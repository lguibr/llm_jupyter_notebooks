{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4afb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python libraries\n",
    "!python3 -m pip install --upgrade langchain chromadb openai gitpython tiktoken python-magic ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import magic\n",
    "import base64\n",
    "import datetime\n",
    "from getpass import getpass\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from langchain.document_loaders import TextLoader \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains.router.multi_retrieval_qa import MultiRetrievalQAChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up API keys\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", getpass(\"OpenAi Token:\"))\n",
    "\n",
    "#Set local path to load files\n",
    "directories = [\n",
    "    (\n",
    "        \"LangChain (framework for developing applications powered by language models) source code and docs\",\n",
    "        \"/home/lg/Lab/langchain\"\n",
    "    ),\n",
    "    (\n",
    "        \"My project the File Chat (jupyter notebook that let you talk with your files) source code and docs\",\n",
    "        \"/home/lg/Lab/jupyter_notebooks/file_chat\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ignored_paths = [\n",
    "    '.git',\n",
    "    '.github',\n",
    "    'venv',\n",
    "    'node_modules',\n",
    "    '.lock',\n",
    "    '.chroma',\n",
    "    '.chroma_db',\n",
    "    '.ipynb_checkpoints',\n",
    "    # Add more paths to ignored here\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_text_file(filepath):\n",
    "    file_type = magic.from_file(filepath)\n",
    "    return 'text' in file_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_collection_name(input_string):\n",
    "    # Define a list of common stop words\n",
    "    stop_words = [\"a\", \"an\", \"the\", \"is\", \"are\", \"am\", \"was\", \"were\", \"be\", \"being\", \"been\", \"to\", \"of\", \"in\", \"on\", \"at\", \"for\", \"with\", \"from\", \"by\", \"and\", \"or\", \"not\", \"but\"]\n",
    "\n",
    "    # Split the input string into words\n",
    "    words = input_string.lower().split(\"/\")\n",
    "\n",
    "    # Remove stop words from the list of words\n",
    "    important_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Combine the important words into a new string with underscores\n",
    "    new_string = \"_\".join(important_words)[-60:]\n",
    "\n",
    "    # Remove all punctuation except underscores from the new string\n",
    "    new_string = \"\".join(c for c in new_string if c.isalnum() or c == \"_\")\n",
    "\n",
    "    return f'db_multi_source{new_string}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set model and timeout\n",
    "\n",
    "model = 'gpt-4-0314'\n",
    "timeout = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chat_openai(model, timeout):\n",
    "    return ChatOpenAI(\n",
    "        model=model,\n",
    "        timeout=timeout,\n",
    "        streaming=True,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "model = initialize_chat_openai(model, timeout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(dir_path, ignored_paths):\n",
    "    docs = []\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        if any(ignored_path in dirpath for ignored_path in ignored_paths):\n",
    "            continue\n",
    "\n",
    "        for file in filenames:\n",
    "            filepath = os.path.join(dirpath, file)\n",
    "            if is_text_file(filepath):\n",
    "                try:\n",
    "                    loader = TextLoader(filepath, encoding='utf-8')\n",
    "                    content = loader.load_and_split()\n",
    "                    docs.extend(content)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading file: {file}, error: {e}\")\n",
    "                    pass\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retrievers(docs, embeddings, collection_name):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=8191, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    db = Chroma.from_documents(documents=texts, embeddings=embeddings, persist_directory=\"chroma_db\")\n",
    "    retriever = db.as_retriever(k=20, collection_name=collection_name)\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tuples = []\n",
    "\n",
    "for dir in directories:\n",
    "    name = dir[0]\n",
    "    dir_path = dir[1]\n",
    "    collection_name = clean_collection_name(dir_path)\n",
    "\n",
    "    print(f\"Loading documents from {dir_path} into collection {collection_name}\")\n",
    "\n",
    "    docs = load_documents(dir_path, ignored_paths)\n",
    "    print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    retriever = create_retrievers(docs, embeddings, collection_name)\n",
    "    retriever_tuples.append((collection_name, f\"Good for answering questions about {name}\", retriever))\n",
    "\n",
    "retriever_names, retriever_descriptions, retrievers = zip(*retriever_tuples)\n",
    "\n",
    "qa = MultiRetrievalQAChain.from_retrievers(\n",
    "    llm=model,\n",
    "    retriever_names=retriever_names,\n",
    "    retriever_descriptions=retriever_descriptions,\n",
    "    retrievers=retrievers,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bcd4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_markdown_download_link(chat_history):\n",
    "    filename = f\"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for q, a in chat_history:\n",
    "            f.write(f\"**Q:** {q}\\n\\n\")\n",
    "            f.write(f\"**A:** {a}\\n\\n\")\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    download_link = f'<a href=\"data:text/markdown;base64,{b64}\" download=\"{filename}\">Download chat history as Markdown</a>'\n",
    "    display(Markdown(download_link))\n",
    "\n",
    "\n",
    "def on_ask_question_button_click(button):\n",
    "    clear_output()\n",
    "    display(chat_interface)\n",
    "    \n",
    "    question = question_input.value.strip()\n",
    "    if question.lower() == 'exit' or question == '':\n",
    "        create_markdown_download_link(chat_history)\n",
    "        return\n",
    "    \n",
    "    answer_data = qa.run(question)\n",
    "    answer = format_answer(answer_data)\n",
    "    chat_history.append((question, answer))\n",
    "    \n",
    "    for q, a in chat_history:\n",
    "        display(Markdown(f\"**Q:** {q}\"))\n",
    "        display(Markdown(f\"**A:** {a}\"))\n",
    "\n",
    "def format_answer(answer_data):\n",
    "    formatted_answer = answer_data.split(\"\\n\")[0]\n",
    "    return formatted_answer\n",
    "\n",
    "# The rest of the chat interface code remains the same\n",
    "\n",
    "question_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your question',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='90%')\n",
    ")\n",
    "\n",
    "ask_question_button = widgets.Button(\n",
    "    description='Ask',\n",
    "    layout=widgets.Layout(width='8%')\n",
    ")\n",
    "\n",
    "ask_question_button.on_click(on_ask_question_button_click)\n",
    "\n",
    "chat_interface = widgets.HBox([question_input, ask_question_button])\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "display(chat_interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usage to write while waiting the model to load the complete answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
