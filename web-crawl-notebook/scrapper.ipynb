{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import os\n",
    "import time\n",
    "from boilerpy3 import extractors\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://docs.trychroma.com\",\n",
    "    \"https://docs.langchain.com/docs\",\n",
    "    \"https://js.langchain.com/docs\",\n",
    "    \"https://python.langchain.com/en/latest\",\n",
    "    # \"https://python.langchain.com/en/latest/getting_started/getting_started.html#\",\n",
    "    # \"https://python.langchain.com/en/latest/modules/models.html\",\n",
    "    # \"https://python.langchain.com/en/latest/modules/prompts.html\", \n",
    "    # \"https://python.langchain.com/en/latest/modules/indexes.html\",\n",
    "    # \"https://python.langchain.com/en/latest/modules/memory.html\",\n",
    "    # \"https://python.langchain.com/en/latest/modules/chains.html\",\n",
    "    # \"https://python.langchain.com/en/latest/modules/agents.html\",\n",
    "    # \"https://python.langchain.com/en/latest/modules/callbacks/getting_started.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/personal_assistants.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/autonomous_agents.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/question_answering.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/chatbots.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/tabular.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/code.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/apis.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/summarization.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/extraction.html\",\n",
    "    # \"https://python.langchain.com/en/latest/use_cases/evaluation.html\",\n",
    "    ]\n",
    "\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "def fetch_url(url, headers, retries=5, backoff_factor=2):\n",
    "    for retry in range(retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            return response\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                wait_time = backoff_factor ** retry\n",
    "                print(f\"HTTP Error 429: Too Many Requests. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            elif e.response.status_code == 404:\n",
    "                print(f\"HTTP Error 404: Not Found. Ignoring URL: {url}\")\n",
    "                return None\n",
    "            else:\n",
    "                print(e)\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return None\n",
    "\n",
    "def get_hyperlinks(url, headers):\n",
    "    response = fetch_url(url, headers)\n",
    "\n",
    "    if response is None or not response.headers.get('Content-Type', '').startswith(\"text/html\"):\n",
    "        return []\n",
    "\n",
    "    html = response.text\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "def get_common_path_prefix(urls):\n",
    "    common_prefixes = {}\n",
    "    for url in urls:\n",
    "        domain = urlparse(url).netloc\n",
    "        path = urlparse(url).path\n",
    "        if domain not in common_prefixes:\n",
    "            common_prefixes[domain] = path\n",
    "        else:\n",
    "            common_prefix = os.path.commonprefix([common_prefixes[domain], path])\n",
    "            common_prefix = common_prefix[:common_prefix.rfind('/')]\n",
    "            common_prefixes[domain] = common_prefix.rstrip('/') + '/'\n",
    "    return common_prefixes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COMMON_PATH_PREFIXES = get_common_path_prefix(urls)\n",
    "\n",
    "\n",
    "def get_domain_hyperlinks(local_domain, url, headers):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url, headers)):\n",
    "        clean_link = None\n",
    "\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain\n",
    "            if local_domain in COMMON_PATH_PREFIXES:\n",
    "                common_prefix = COMMON_PATH_PREFIXES[local_domain]\n",
    "                if not link.startswith(common_prefix):\n",
    "                    if not clean_link.endswith(\"/\") and not common_prefix.startswith(\"/\"):\n",
    "                        clean_link += \"/\"\n",
    "                    clean_link += common_prefix\n",
    "            if not clean_link.endswith(\"/\") and not link.startswith(\"/\"):\n",
    "                clean_link += \"/\"\n",
    "            clean_link += link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    return list(set(clean_links))\n",
    "\n",
    "def get_text_from_url(url, headers):\n",
    "    response = fetch_url(url, headers)\n",
    "    if response is None:\n",
    "        return None\n",
    "\n",
    "    # Use boilerpy3 to extract main content\n",
    "    extractor = extractors.DefaultExtractor()\n",
    "    text = extractor.get_content(response.text)\n",
    "\n",
    "    # Ignore pages with specific text patterns\n",
    "    ignore_patterns = [\"NOT FOUND\", \"404\",\"  404 Not Found\", \"Page Not Found\", \"We could not find what you were looking for.\"]\n",
    "    if any(pattern in text for pattern in ignore_patterns):\n",
    "        print(f\"Ignoring URL (matched ignore pattern): {url}\")\n",
    "        return None\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_to_file(local_domain, url, text):\n",
    "    if not text.strip():  # Check if the text is empty or contains only whitespaces\n",
    "        return\n",
    "\n",
    "    file_path = f\"text/{local_domain}/{url[8:].replace('/', '_')}.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "def create_directories(local_domain):\n",
    "    if not os.path.exists(\"text/\"):\n",
    "        os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(f\"text/{local_domain}/\"):\n",
    "        os.mkdir(f\"text/{local_domain}/\")\n",
    "\n",
    "def crawl(url):\n",
    "    local_domain = urlparse(url).netloc\n",
    "    queue = deque([url])\n",
    "    seen = set([url])\n",
    "    seen_paths = set([urlparse(url).path])\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    create_directories(local_domain)\n",
    "\n",
    "    while queue:\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        text = get_text_from_url(url, headers)\n",
    "        if text is None:\n",
    "            continue\n",
    "\n",
    "        save_text_to_file(local_domain, url, text)\n",
    "\n",
    "        for link in get_domain_hyperlinks(local_domain, local_domain, url, headers):\n",
    "            link_path = urlparse(link).path\n",
    "            if link not in seen and link_path not in seen_paths:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "                seen_paths.add(link_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    crawl(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
